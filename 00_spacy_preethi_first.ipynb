{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
=======
   "execution_count": null,
   "metadata": {},
>>>>>>> 042e4495359d4d6b5708d658d652bbbbac715f9e
   "outputs": [],
   "source": [
    "# Import spacy and English models\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process text"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = nlp(u'')"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process sentences 'Hello, world. Natural Language Processing in 10 lines of code.' using spaCy\n",
    "doc = nlp(u'Hello, world. Natural Language Processing in 10 lines of code.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get tokens and sentences\n",
    "\n",
    "#### What is a Token?\n",
    "A token is a single chopped up element of the sentence, which could be a word or a group of words to analyse. The task of chopping the sentence up is called \"tokenisation\".\n",
    "\n",
    "Example: The following sentence can be tokenised by splitting up the sentence into individual words.\n",
    "\n",
    "\t\"Cytora is going to PyCon!\"\n",
    "\t[\"Cytora\",\"is\",\"going\",\"to\",\"PyCon!\"]"
>>>>>>> 042e4495359d4d6b5708d658d652bbbbac715f9e
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What\n",
      "What's up Vienna?\n",
      "Let's teach computers to understand us.\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 042e4495359d4d6b5708d658d652bbbbac715f9e
   "source": [
    "# Get first token of the processed document\n",
    "token = doc[0]\n",
    "print(token)\n",
    "\n",
    "# Print sentences (one sentence per line)\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What - NOUN\n",
      "'s - VERB\n",
      "up - ADP\n",
      "Vienna - PROPN\n",
      "? - PUNCT\n",
      "Let - VERB\n",
      "'s - PRON\n",
      "teach - VERB\n",
      "computers - NOUN\n",
      "to - PART\n",
      "understand - VERB\n",
      "us - PRON\n",
      ". - PUNCT\n"
     ]
    }
   ],
=======
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech tags\n",
    "\n",
    "#### What is a Speech Tag?\n",
    "A speech tag is a context sensitive description of what a word means in the context of the whole sentence.\n",
    "More information about the kinds of speech tags which are used in NLP can be [found here](http://www.winwaed.com/blog/2011/11/08/part-of-speech-tags/).\n",
    "\n",
    "Examples:\n",
    "\n",
    "1. CARDINAL, Cardinal Number - 1,2,3\n",
    "2. PROPN, Proper Noun, Singular - \"Matic\", \"Andraz\", \"Cardiff\"\n",
    "3. INTJ, Interjection - \"Uhhhhhhhhhhh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 042e4495359d4d6b5708d658d652bbbbac715f9e
   "source": [
    "# For each token, print corresponding part of speech tag\n",
    "for token in doc:\n",
    "    print('{} - {}'.format(token, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual part of speech tagging ([displaCy](https://displacy.spacy.io))"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What --> [What, 's]\n",
      "'s --> []\n",
      "up --> [up, 's]\n",
      "Vienna --> [Vienna, 's]\n",
      "? --> [?, 's]\n",
      "Let --> []\n",
      "'s --> ['s, teach, teach, Let]\n",
      "teach --> [teach, Let]\n",
      "computers --> [computers, teach, teach, Let]\n",
      "to --> [to, understand, understand, teach, teach, Let]\n",
      "understand --> [understand, teach, teach, Let]\n",
      "us --> [us, understand, understand, teach, teach, Let]\n",
      ". --> [., Let]\n",
      "What-nsubj-> 's-ROOT\n",
      "\n",
      "up-prep-> 's-ROOT\n",
      "Vienna-nsubj-> 's-ROOT\n",
      "?-punct-> 's-ROOT\n",
      "\n",
      "'s-nsubj-> teach-ccomp-> teach-ccomp-> Let-ROOT\n",
      "teach-ccomp-> Let-ROOT\n",
      "computers-dobj-> teach-ccomp-> teach-ccomp-> Let-ROOT\n",
      "to-aux-> understand-xcomp-> understand-xcomp-> teach-ccomp-> teach-ccomp-> Let-ROOT\n",
      "understand-xcomp-> teach-ccomp-> teach-ccomp-> Let-ROOT\n",
      "us-dobj-> understand-xcomp-> understand-xcomp-> teach-ccomp-> teach-ccomp-> Let-ROOT\n",
      ".-punct-> Let-ROOT\n"
     ]
    }
   ],
=======
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic dependencies\n",
    "\n",
    "#### What are syntactic dependencies?\n",
    "\n",
    "We have the speech tags and we have all of the tokens in a sentence, but how do we relate the two to uncover the syntax in a sentence? Syntactic dependencies describe how each type of word relates to each other in a sentence, this is important in NLP in order to extract structure and understand grammar in plain text.\n",
    "\n",
    "Example:\n",
    "\n",
    "<img src=\"images/syntax-dependencies-oliver.png\" align=\"left\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 042e4495359d4d6b5708d658d652bbbbac715f9e
   "source": [
    "# Write a function that walks up the syntactic tree of the given token and collects all tokens to the root token (including root token).\n",
    "\n",
    "def tokens_to_root(token):\n",
    "    \"\"\"\n",
    "    Walk up the syntactic tree, collecting tokens to the root of the given `token`.\n",
    "    :param token: Spacy token\n",
    "    :return: list of Spacy tokens\n",
    "    \"\"\"\n",
    "    tokens_to_r = []\n",
    "    while token.head is not token:\n",
    "        tokens_to_r.append(token)\n",
    "        token = token.head\n",
    "        tokens_to_r.append(token)\n",
    "\n",
    "    return tokens_to_r\n",
    "\n",
    "# For every token in document, print it's tokens to the root\n",
    "for token in doc:\n",
    "    print('{} --> {}'.format(token, tokens_to_root(token)))\n",
    "\n",
    "# Print dependency labels of the tokens\n",
    "for token in doc:\n",
    "    print('-> '.join(['{}-{}'.format(dependent_token, dependent_token.dep_) for dependent_token in tokens_to_root(token)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vienna - GPE\n",
      "JavaScript - ORG\n"
     ]
    }
   ],
   "source": [
    "# Print all named entities with named entity types\n",
    "\n",
    "doc_2 = nlp(u\"I went to Vienna to meet some really awesome JavaScript friends.\")\n",
    "for ent in doc_2.ents:\n",
    "    print('{} - {}'.format(ent, ent.label_))"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, Vienna, some really awesome JavaScript friends]\n"
     ]
    }
   ],
=======
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun chunks\n",
    "\n",
    "#### What is a Noun Chunk?\n",
    "Noun chunks are the phrases based upon nouns recovered from tokenized text using the speech tags.\n",
    "\n",
    "Example:\n",
    "\n",
    "The sentence \"The boy saw the yellow dog\" has 2 noun objects, the boy and the dog. \n",
    "Therefore the noun chunks will be\n",
    "\n",
    "\t1. \"The boy\"\n",
    "\t2. \"the yellow dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 042e4495359d4d6b5708d658d652bbbbac715f9e
   "source": [
    "# Print noun chunks for doc_2\n",
    "print([chunk for chunk in doc_2.noun_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram probabilities"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I , -4.064180850982666\n",
      "went , -8.474893569946289\n",
      "to , -3.83851957321167\n",
      "Vienna , -19.579313278198242\n",
      "to , -3.83851957321167\n",
      "meet , -9.823533058166504\n",
      "some , -6.4027814865112305\n",
      "really , -6.664026737213135\n",
      "awesome , -8.797789573669434\n",
      "JavaScript , -19.579313278198242\n",
      "friends , -8.6137056350708\n",
      ". , -3.0729479789733887\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 042e4495359d4d6b5708d658d652bbbbac715f9e
   "source": [
    "# For every token in doc_2, print log-probability of the word, estimated from counts from a large corpus \n",
    "for token in doc_2:\n",
    "    print(token, ',', token.prob)"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "\n",
      "0.445694088466\n",
      "0.235097658138\n"
     ]
    }
   ],
   "source": [
    "# For a given document, calculate similarity between 'apples' and 'oranges' and 'boots' and 'hippos'\n",
    "doc = nlp(u\"I went to Vienna on an airplane. I met some awesome JavaScript and Python friends.\")\n",
    "vienna = doc[3]\n",
    "airplane = doc[6]\n",
    "javascript = doc[10]\n",
    "python = doc[12]\n",
    "print(vienna.similarity(airplane))\n",
    "print(javascript.similarity(python))\n",
    "\n",
    "print()\n",
    "# Print similarity between sentence and word 'fruit'\n",
    "vienna, friends = doc.sents\n",
    "city = doc.vocab[u'city']\n",
    "coding = doc.vocab[u'coding']\n",
    "print(vienna.similarity(city))\n",
    "print(friends.similarity(coding))"
=======
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding / Similarity\n",
    "\n",
    "#### What are Word embeddings?\n",
    "\n",
    "A word embedding is a representation of a word, and by extension a whole language corpus, in a vector or other form of numerical mapping. This allows words to be treated numerically with word similarity represented as spatial difference in the dimensions of the word embedding mapping.\n",
    "\n",
    "Example:\n",
    "\t\n",
    "With word embeddings we can understand that vector operations describe word similarity. This means that we can see vector proofs of statements such as:\n",
    "\n",
    "\tking-queen==man-woman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a given document, calculate similarity between 'apples' and 'oranges' and 'boots' and 'hippos'\n",
    "doc = nlp(u\"Apples and oranges are similar. Boots and hippos aren't.\")\n",
    "apples = doc[0]\n",
    "oranges = doc[2]\n",
    "boots = doc[6]\n",
    "hippos = doc[8]\n",
    "print(apples.similarity(oranges))\n",
    "print(boots.similarity(hippos))\n",
    "\n",
    "print()\n",
    "# Print similarity between sentence and word 'fruit'\n",
    "apples_sent, boots_sent = doc.sents\n",
    "fruit = doc.vocab[u'fruit']\n",
    "print(apples_sent.similarity(fruit))\n",
    "print(boots_sent.similarity(fruit))"
>>>>>>> 042e4495359d4d6b5708d658d652bbbbac715f9e
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
<<<<<<< HEAD
    "version": 3
=======
    "version": 2
>>>>>>> 042e4495359d4d6b5708d658d652bbbbac715f9e
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
<<<<<<< HEAD
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
=======
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
>>>>>>> 042e4495359d4d6b5708d658d652bbbbac715f9e
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
