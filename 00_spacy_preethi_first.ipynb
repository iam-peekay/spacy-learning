{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import spacy and English models\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = nlp(u'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What\n",
      "What's up Vienna?\n",
      "Let's teach computers to understand us.\n"
     ]
    }
   ],
   "source": [
    "# Get first token of the processed document\n",
    "token = doc[0]\n",
    "print(token)\n",
    "\n",
    "# Print sentences (one sentence per line)\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What - NOUN\n",
      "'s - VERB\n",
      "up - ADP\n",
      "Vienna - PROPN\n",
      "? - PUNCT\n",
      "Let - VERB\n",
      "'s - PRON\n",
      "teach - VERB\n",
      "computers - NOUN\n",
      "to - PART\n",
      "understand - VERB\n",
      "us - PRON\n",
      ". - PUNCT\n"
     ]
    }
   ],
   "source": [
    "# For each token, print corresponding part of speech tag\n",
    "for token in doc:\n",
    "    print('{} - {}'.format(token, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual part of speech tagging ([displaCy](https://displacy.spacy.io))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What --> [What, 's]\n",
      "'s --> []\n",
      "up --> [up, 's]\n",
      "Vienna --> [Vienna, 's]\n",
      "? --> [?, 's]\n",
      "Let --> []\n",
      "'s --> ['s, teach, teach, Let]\n",
      "teach --> [teach, Let]\n",
      "computers --> [computers, teach, teach, Let]\n",
      "to --> [to, understand, understand, teach, teach, Let]\n",
      "understand --> [understand, teach, teach, Let]\n",
      "us --> [us, understand, understand, teach, teach, Let]\n",
      ". --> [., Let]\n",
      "What-nsubj-> 's-ROOT\n",
      "\n",
      "up-prep-> 's-ROOT\n",
      "Vienna-nsubj-> 's-ROOT\n",
      "?-punct-> 's-ROOT\n",
      "\n",
      "'s-nsubj-> teach-ccomp-> teach-ccomp-> Let-ROOT\n",
      "teach-ccomp-> Let-ROOT\n",
      "computers-dobj-> teach-ccomp-> teach-ccomp-> Let-ROOT\n",
      "to-aux-> understand-xcomp-> understand-xcomp-> teach-ccomp-> teach-ccomp-> Let-ROOT\n",
      "understand-xcomp-> teach-ccomp-> teach-ccomp-> Let-ROOT\n",
      "us-dobj-> understand-xcomp-> understand-xcomp-> teach-ccomp-> teach-ccomp-> Let-ROOT\n",
      ".-punct-> Let-ROOT\n"
     ]
    }
   ],
   "source": [
    "# Write a function that walks up the syntactic tree of the given token and collects all tokens to the root token (including root token).\n",
    "\n",
    "def tokens_to_root(token):\n",
    "    \"\"\"\n",
    "    Walk up the syntactic tree, collecting tokens to the root of the given `token`.\n",
    "    :param token: Spacy token\n",
    "    :return: list of Spacy tokens\n",
    "    \"\"\"\n",
    "    tokens_to_r = []\n",
    "    while token.head is not token:\n",
    "        tokens_to_r.append(token)\n",
    "        token = token.head\n",
    "        tokens_to_r.append(token)\n",
    "\n",
    "    return tokens_to_r\n",
    "\n",
    "# For every token in document, print it's tokens to the root\n",
    "for token in doc:\n",
    "    print('{} --> {}'.format(token, tokens_to_root(token)))\n",
    "\n",
    "# Print dependency labels of the tokens\n",
    "for token in doc:\n",
    "    print('-> '.join(['{}-{}'.format(dependent_token, dependent_token.dep_) for dependent_token in tokens_to_root(token)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vienna - GPE\n",
      "JavaScript - ORG\n"
     ]
    }
   ],
   "source": [
    "# Print all named entities with named entity types\n",
    "\n",
    "doc_2 = nlp(u\"I went to Vienna to meet some really awesome JavaScript friends.\")\n",
    "for ent in doc_2.ents:\n",
    "    print('{} - {}'.format(ent, ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, Vienna, some really awesome JavaScript friends]\n"
     ]
    }
   ],
   "source": [
    "# Print noun chunks for doc_2\n",
    "print([chunk for chunk in doc_2.noun_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I , -4.064180850982666\n",
      "went , -8.474893569946289\n",
      "to , -3.83851957321167\n",
      "Vienna , -19.579313278198242\n",
      "to , -3.83851957321167\n",
      "meet , -9.823533058166504\n",
      "some , -6.4027814865112305\n",
      "really , -6.664026737213135\n",
      "awesome , -8.797789573669434\n",
      "JavaScript , -19.579313278198242\n",
      "friends , -8.6137056350708\n",
      ". , -3.0729479789733887\n"
     ]
    }
   ],
   "source": [
    "# For every token in doc_2, print log-probability of the word, estimated from counts from a large corpus \n",
    "for token in doc_2:\n",
    "    print(token, ',', token.prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "\n",
      "0.445694088466\n",
      "0.235097658138\n"
     ]
    }
   ],
   "source": [
    "# For a given document, calculate similarity between 'apples' and 'oranges' and 'boots' and 'hippos'\n",
    "doc = nlp(u\"I went to Vienna on an airplane. I met some awesome JavaScript and Python friends.\")\n",
    "vienna = doc[3]\n",
    "airplane = doc[6]\n",
    "javascript = doc[10]\n",
    "python = doc[12]\n",
    "print(vienna.similarity(airplane))\n",
    "print(javascript.similarity(python))\n",
    "\n",
    "print()\n",
    "# Print similarity between sentence and word 'fruit'\n",
    "vienna, friends = doc.sents\n",
    "city = doc.vocab[u'city']\n",
    "coding = doc.vocab[u'coding']\n",
    "print(vienna.similarity(city))\n",
    "print(friends.similarity(coding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
